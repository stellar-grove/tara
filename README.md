# ticondagrova Advanced Research & Analytics
repo for code and data pushed by tARA

## Contents
The code contained in this repo is code used to complete the analysis done for our clients and other projects done.
To see the full write ups, etc. see https://www.stellargrove.com/tara.  

### Folders
1. **data**: folder containing different data sets.  There is a separate folder for data used in the pll file.  This folder is aptly named pll.  In this folder there is several.csv and .txt files along with a few .xlsx files.  
2. **notebooks**: Jupyter Notebooks to run the different analyses / projects that is contained in the files below.  A typical structure of this repo is to have a code in one or more files that can be accessed in a Jupyter notebook, allowing uses to have a somewhat interactive experience with the data / analysis.
3. **SongroveBotanicals**: this folder has a module named research in it that houses several different studies performed with our client - Songrove Botanicals.  Songrove is a small agriculture technology firm that is looking to advance the use of statistics, data science and artficial intelligence in the field of agriculture.  The **data** folder holds all the data needed to run the analyses we performed for them, **notebooks** folder holds all the Jupyter notebooks that are used to guide users through the analyses in an organized, sequential way.
4. **sqls**: this folder holds any sql files that need to be run as a part of any of the analyses.
5. **tests**: this folder holds all the files necessary to run the GitHub pytest workflow.  All tests for all the files within tara are stored here.

### Modules
1. **distirbutions.py**: This module holds several different things:
    1. **TSP:**: First it holds the distribution for the Two-Sided-Power Distribution as described by Van Dorp 2004.  
    To use this function you call TSP and provide the low, middle and upper bounds as well as the parameter n.  See the research for more on what each mean.  
    With the function **TSP** you can generate measures of central tendency using the TSP distribution.  It will return a dictionary with the expected value, variance, alpha & beta values needed to perform the cdf function as well as the p and q values generated.      
    2. **createTSPSample:** returns a two-columned data_frame that holds the sample from the uniform distribution to generate the TSP value, and then the TSP value itself.  
        For example, given the parameter list [low,mid,hi,n] = [11,15,22,2] and a size of 100, the **createTSPSample** function would return a DataFrame with two columns that contained values like [0.403533, 15.2231].  Finally there is a helper functin for the TSP distribution, which is **checkTSPParams**.  This is inteneded for the PERT simulator to come.  It simply returns "len not correct" if the length of the list is not equal to 4.
    3. **generateTSP:** Finally, calling the function **generateTSP** generates the a singular value from the Two Sided Power (TSP) distribution. It returns the random uniform sample taken to determine what TSP value it corresponds to, and then also returns the value extracted from the TSP distribution, based on the random uniform sample taken.  Again, see Van Dorp 2004 for more details. 
    2. **sum_product**: this function finds the sum product of two arrays a, and b.  The function is literally return np.sum(a * b).  Error handling will come in a DevOps sprint later in the summer.
    3. **getDefaultParameters**: this function returns commonly used values for different distributions.  The distributions captured are: tsp, normal, poisson, binomial, bernoulli, and sample size.  The values are returned in a dictionary.
    4. **getDataFrameNames**: helper function to return different forms of DataFrame.  The list includes: dataframe, df, data-frame.  More to come later.
    5. **DaCountDeMonteCarlo**:  this is a monte carlo simulator of sorts.  What is meant to do is generate datasets for you.  
    There are a two main types of functions on this class: **sample_** and **create_** functions.  
    Functions that begin with **sample_** are functions that sample from the distribution given.  For example **sampleFromNormal(mean, std, sample)** returns a DataFrame with data that is from a normally distributed variable with average value of *mean*, standard deviation of *std*, and a sample from the [0,1] uniform distribution.  
    **create_** functions however are designed to return sets of data, with rows equal to the *sampleSize* parameter in each data set.  The data contained in the sample is of the distribution specified with the parameters given.  For example, **createUniformData(a, b, sampleSize, output="list")** would return a *list* of size *sampleSize*, which is from a uniformly distributed random variable with low values of *a* and high values of *b*.  This format is followed for each of the different **create_** functions in the DaCountDeMonteCarlo class.  **Note:** the class is usually abbreviated as dcmc when imported into a file.
2. **grovebot.py:**: this is the module that will contain any code done for robotics programming with the kids.  It's an empty shell of a file, as we haven't started construction yet, but will be filled out over time.
3. **hub.py**:  This file is the file that contains all the examples shown for tara's research portfoliio.  The idea behind this file is to have a file that contains all the work from tara in one place that you can call.  Typical structure is that there are a series of functions / procedures held within the different classes that correspond to the different projects done.  The functions are executed from a Jupyter Notebook, which walks the user through the process of building the model and seeing the output.  Classes currently include:
    1. **CreditScore:** class creates data to build a logistic regression model to determine the percentage chance that a customer with a set of given attributes will default on their credit card or not.
    2. **Titanic:** **IN PROGRESS**  this class takes the Titanic dataset from Kaggle and does analysis on it.  I was in the middle of working on this, when I was 
4. **meta.py**: work I was doing for a take home challenge. Some decent stuff in here, but somewhat specialized, so not sure how much it will mean to anyone.
5. **pll.py:** this file contains a few different functions and classes to gather, clean and analyze data pertaining to the Premiere Lacrosse League (PLL).  See below for a listing of the file's contents.
    1. **variables:** variables used throughout the module.  The idea behind this is to gather variables that are used throughout the entire module in one place, so if anything needs to be changed you are doing it in one place, not multiple places throughout the file.  Examples of some variables are: *data_location*: the folder in the repo were data is stored, *DB*: a dictionary that contains the parameters needed to connect to, in this case, a local instance of MSSQLServer.
    2. **helper functions:**: these functions are resuable functions like *get_database* which is a function that you feed the name of a file .csv file that you want to load into a DataFrame to do work on.
    3. **Data:** this class handles the processing of data to get it into a usable form.  This is where examples of ELT and ETL can be shown.  Much of the data that is in here is surrounding player statistics, attendance, and then some made up data to do analysis on.  The class also contains a function called *run_data_cleaner*.  This is a function that runs a series of smaller, more specialized processes.  The processes contain things like checking if the first rows of a DataFrame are all blank, signalling the need for the *skiprows* parameter.  Other functions include turning numbers stored as strings into decimals or integers, determine the number of null values in each column, look for character values in numeric columns, etc.
    4. **simulator:** this class has a series of functions and processes that generate simulated data for customer attributes and other things.  The idea behind this class is to show how, given rules, you can simulate out possible scenarios, then analyze said scenarios over and over again, gather what is hopefully a more accurate picture of the problem you are looking to analyze.
    5. **analysis:** this class shows off some of the heavier Data Science / Machine Learning techniques like clustering, classification, modelling and predictions.  ML gets involved by automating these individual analyses to create an automated analysis.
6. **stuffs.py:**  this file holds a bunch of constans like connection strings, data locations etc. It's a way to put everything that is commonly referenced in one place for maintenance, though some of the modules have their own variables set, so **stuffs.py** isn't used.
